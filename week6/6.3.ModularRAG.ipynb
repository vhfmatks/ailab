{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install langgraph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ModularRAG \n",
    "[[https://arxiv.org/pdf/2407.21059]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_postgres import PGVector\n",
    "import os \n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)    \n",
    "embeddings = OpenAIEmbeddings()\n",
    "\n",
    "vectorstore = PGVector(\n",
    "    collection_name=\"chapter6_collection\",\n",
    "    embeddings=embeddings,\n",
    "    connection=f\"postgresql://{os.getenv('POSTGRES_USER')}:{os.getenv('POSTGRES_PASSWORD')}@{os.getenv('PGVECTOR_HOST')}:{os.getenv('PGVECTOR_PORT')}/{os.getenv('POSTGRES_DB')}\",\n",
    "    use_jsonb=True,\n",
    ")\n",
    "retriever = vectorstore.as_retriever()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "# 문서 검색 필요 여부 검사\n",
    "class NeedRetrieval(BaseModel):\n",
    "    need_retrieval: str = Field(description=\"문서 검색이 필요한지 여부(Y or N)\")\n",
    "    reason: str = Field(description=\"문서 검색이 필요하거나 필요하지 않은 이유\")\n",
    "\n",
    "structured_llm = llm.with_structured_output(NeedRetrieval)\n",
    "\n",
    "system = \"\"\"\n",
    "당신은 사용자의 질문을 분석하여 문서 검색이 필요한지 여부를 판단하는 역할을 합니다.\n",
    "다음과 같은 경우에는 문서 검색이 필요합니다:\n",
    "- 사실 기반 정보를 요구하는 질문\n",
    "- 특정 데이터나 통계를 요구하는 질문\n",
    "- 역사적 사건이나 날짜에 관한 질문\n",
    "\n",
    "다음과 같은 경우에는 문서 검색이 필요하지 않습니다:\n",
    "- 일반적인 대화나 인사\n",
    "- 개인적인 의견을 묻는 질문\n",
    "- 창의적인 내용 생성 요청\n",
    "- 간단한 계산이나 논리적 추론만 필요한 질문\n",
    "\n",
    "질문을 분석하고 문서 검색이 필요한지 여부와 그 이유를 제공해주세요.\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", system),\n",
    "    (\"user\", \"{question}\")\n",
    "])\n",
    "\n",
    "need_retrieval_chain = prompt | structured_llm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "need_retrieval_chain.invoke(\"안녕하세요?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "class GradeDocuments(BaseModel):\n",
    "    score : int = Field(description=\"The score of the document(from 0 to 10)\")\n",
    "\n",
    "structured_llm = llm.with_structured_output(GradeDocuments)\n",
    "\n",
    "system = \"\"\"\n",
    "당신은 문서와 질문을 받아서 문서의 정확도를 평가하는 역할을 합니다.\n",
    "문서의 정확도를 평가하는 기준은 다음과 같습니다.\n",
    "- 문서가 질문에 대해 정확하게 답변하였는가?\n",
    "- 문서가 질문에 대해 중요한 내용을 포함하였는가?\n",
    "- 문서가 질문에 대해 불필요한 내용을 포함하였는가?\n",
    "\n",
    "문서의 정확도를 0부터 10까지의 점수로 평가해주세요.\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", system),\n",
    "    (\"user\", \"문서 : {documents}\"), \n",
    "    (\"user\", \"질문 : {question}\"),\n",
    "])\n",
    "\n",
    "\n",
    "grade_documents_chain = prompt | structured_llm "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grade_documents_chain.invoke({\"documents\": \"일시불 결제 시 캐시백 해줍니다.\", \"question\": \"할부전환에 필요한 조건은 무엇인가?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "system = \"\"\"\n",
    "당신은 신용카드사의 친절한 상담사 입니다.\n",
    "사용자의 질문에 대해 친절하게 답변해주세요.\n",
    "문서가 제공되는 경우 문서의 내용을 참고하여 답변해주세요.\n",
    "\"\"\"\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", system),\n",
    "    (\"user\", \"질문: {question}\"),\n",
    "    (\"user\", \"문서: {documents}\")\n",
    "])\n",
    "\n",
    "def format_docs(docs):\n",
    "    return \"\\n\".join([f\"문서 {i+1}: {doc.page_content}\" for i, doc in enumerate(docs)]) \n",
    "\n",
    "generate_chain = prompt | llm | StrOutputParser()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "class Groundedness(BaseModel):\n",
    "    score: int = Field(description=\"문서의 근거성 점수(0부터 10까지)\")\n",
    "    explanation: str = Field(description=\"점수에 대한 설명\")\n",
    "\n",
    "structured_llm = llm.with_structured_output(Groundedness)\n",
    "\n",
    "system = \"\"\"\n",
    "당신은 사용자의 질문과 LLM의 응답을 받아서 응답의 근거성(groundedness)을 평가하는 역할을 합니다.\n",
    "근거성이란 응답이 사용자의 질문과 문서에 기반하여 작성되었는지를 의미합니다.\n",
    "\n",
    "근거성을 평가하는 기준은 다음과 같습니다:\n",
    "- 응답의 내용이 사용자의 질문과 문서에 명시적으로 언급되어 있는가?\n",
    "- 응답이 사용자의 질문과 문서의 내용을 왜곡하거나 과장하지 않았는가?\n",
    "- 응답이 사용자의 질문과 문서에 없는 내용을 추가하지 않았는가?\n",
    "\n",
    "근거성을 0부터 10까지의 점수로 평가하고, 그 이유를 설명해주세요.\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", system),\n",
    "    (\"user\", \"질문: {question}\"),\n",
    "    (\"user\", \"문서: {documents}\"),\n",
    "    (\"user\", \"응답: {response}\"),\n",
    "])\n",
    "\n",
    "groundedness_chain = prompt | structured_llm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "groundedness_chain.invoke(\n",
    "    {\"question\": \"할부전환에 필요한 조건은 무엇인가?\",\n",
    "     \"documents\": \"일시불 결제 시 캐시백 해줍니다.\", \n",
    "     \"response\": \"1개월 이전 승인건만 가능합니다\"}\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "class QueryRewrite(BaseModel):\n",
    "    rewritten_query: str = Field(description=\"재작성된 쿼리\")\n",
    "    explanation: str = Field(description=\"쿼리 재작성 이유 설명\")\n",
    "\n",
    "structured_llm = llm.with_structured_output(QueryRewrite)\n",
    "\n",
    "system = \"\"\"\n",
    "당신은 사용자의 질문을 분석하고 더 효과적인 검색을 위해 질문을 재작성하는 역할을 합니다.\n",
    "원래 질문의 의도를 유지하면서 검색 엔진에서 더 관련성 높은 결과를 얻을 수 있도록 질문을 재작성해주세요.\n",
    "\n",
    "쿼리 재작성 시 고려할 사항:\n",
    "- 불필요한 단어나 문구 제거\n",
    "- 핵심 키워드 강조\n",
    "- 동의어나 관련 용어 추가\n",
    "- 질문의 의도를 명확히 표현\n",
    "\n",
    "재작성된 쿼리와 재작성 이유를 함께 제공해주세요.\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", system),\n",
    "    (\"user\", \"원래 질문: {question}\"),\n",
    "])\n",
    "\n",
    "query_rewrite_chain = prompt | structured_llm \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_rewrite_chain.invoke({\"question\": \"할부전환에 필요한 조건은 무엇인가?\"})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Annotated, List\n",
    "from typing_extensions import TypedDict\n",
    "\n",
    "class GraphState(TypedDict):\n",
    "    question : Annotated[str, \"사용자의 질문\"]\n",
    "    documents: Annotated[List[str], \"검색된 문서\"]\n",
    "    generation : Annotated[str, \"LLM의 응답\"]\n",
    "    have_to_retrieve : Annotated[str, \"문서 검색 필요 여부\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 노드 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def need_retrieval(state:GraphState) -> GraphState:\n",
    "    print(\"NEED RETRIEVAL....\")\n",
    "    question = state[\"question\"]\n",
    "    have_to_retrieve = need_retrieval_chain.invoke({\n",
    "        \"question\": question\n",
    "    })\n",
    "    return {'have_to_retrieve': have_to_retrieve.need_retrieval, 'documents':[]}\n",
    "\n",
    "## 문서 검색\n",
    "def retrieve(state:GraphState) -> GraphState:\n",
    "    print(\"RETRIEVE....\")\n",
    "    question = state[\"question\"]\n",
    "    documents = retriever.invoke(question)\n",
    "    return {'documents': documents}\n",
    "\n",
    "## 문서 평가\n",
    "def grade_documents(state:GraphState) -> GraphState:\n",
    "    print(\"GRADE DOCUMENTS....\")\n",
    "    question = state[\"question\"]\n",
    "    documents = state[\"documents\"]\n",
    "    \n",
    "    filtered_docs = []\n",
    "    for d in documents:\n",
    "        score = grade_documents_chain.invoke({\n",
    "            \"documents\": d,\n",
    "            \"question\": question\n",
    "        })\n",
    "        if score.score >= 6:\n",
    "            filtered_docs.append(d)\n",
    "    \n",
    "    return {'documents': filtered_docs}\n",
    "\n",
    "## 응답 생성\n",
    "def generate(state:GraphState) -> GraphState:\n",
    "    print(\"GENERATE....\")\n",
    "    question = state[\"question\"]\n",
    "    documents = state[\"documents\"]\n",
    "\n",
    "    generation = generate_chain.invoke({\n",
    "        \"question\": question,\n",
    "        \"documents\": format_docs(documents)\n",
    "    })\n",
    "    return {'generation': generation}\n",
    "\n",
    "## 쿼리 재작성\n",
    "def rewrite_query(state:GraphState) -> GraphState:\n",
    "    print(\"REWRITE QUERY....\")\n",
    "    question = state[\"question\"]\n",
    "    rewritten_query = query_rewrite_chain.invoke({\n",
    "        \"question\": question\n",
    "    })\n",
    "    return {'question': rewritten_query.rewritten_query}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 조건부 엣지 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## 문서 검색 필요 여부 검사\n",
    "def decide_need_retrieval(state:GraphState) -> GraphState:\n",
    "    print(\"DECISION NEED RETRIEVAL....\")\n",
    "    have_to_retrieve = state[\"have_to_retrieve\"]\n",
    "\n",
    "    if have_to_retrieve == \"Y\":\n",
    "        return 'retrieve'\n",
    "    else:\n",
    "        return 'generate'\n",
    "\n",
    "\n",
    "def decide_groundedness(state:GraphState) -> GraphState:\n",
    "    print(\"DECISION GROUNDEDNESS....\")\n",
    "    question = state[\"question\"]\n",
    "    documents = state[\"documents\"]\n",
    "    generation = state[\"generation\"]\n",
    "\n",
    "    \n",
    "    groundedness = groundedness_chain.invoke({\n",
    "        \"question\": question,\n",
    "        \"documents\": documents,\n",
    "        \"response\": generation\n",
    "    })\n",
    "    if groundedness.score >= 5:\n",
    "        return 'end'\n",
    "    else:\n",
    "        return 'rewrite_query'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 그래프 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import StateGraph, START, END\n",
    "\n",
    "workflow = StateGraph(GraphState)\n",
    "\n",
    "workflow.add_node(\"need_retrieval\", need_retrieval)\n",
    "workflow.add_node(\"retrieve\", retrieve)\n",
    "workflow.add_node(\"grade_documents\", grade_documents)\n",
    "workflow.add_node(\"generate\", generate)\n",
    "workflow.add_node(\"rewrite_query\", rewrite_query)\n",
    "\n",
    "workflow.add_edge(START, \"need_retrieval\")\n",
    "workflow.add_conditional_edges(\n",
    "    \"need_retrieval\",\n",
    "    decide_need_retrieval,\n",
    "    {\n",
    "        'retrieve': 'retrieve',\n",
    "        'generate': 'generate'\n",
    "    }\n",
    ")\n",
    "workflow.add_edge('retrieve', 'grade_documents')\n",
    "workflow.add_edge('grade_documents', 'generate')\n",
    "\n",
    "workflow.add_conditional_edges(\n",
    "    'generate',\n",
    "    decide_groundedness,\n",
    "    {\n",
    "        'end': END,\n",
    "        'rewrite_query': 'rewrite_query'\n",
    "    }\n",
    ")\n",
    "\n",
    "workflow.add_edge('rewrite_query', \"need_retrieval\")\n",
    "\n",
    "app = workflow.compile()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print ( app.get_graph().draw_mermaid() )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnableConfig\n",
    "import uuid\n",
    "config = RunnableConfig(recursion_limit=15, configurable={'thread_id':uuid.uuid4()})\n",
    "response = app.invoke({'question' :\"적립서비스가 제공되는 전월 실적 기준은 얼마인가요?\"}, config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
